{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "# Plots the evolution of the training accuracy and loss\n",
    "#\n",
    "def plot(h,epochs):\n",
    "    LOSS = 0; ACCURACY = 1\n",
    "    training = np.zeros((2,epochs)); testing = np.zeros((2,epochs))\n",
    "    training[LOSS] = h.history['loss']\n",
    "    testing[LOSS] = h.history['val_loss']    # validation loss\n",
    "    training[ACCURACY] = h.history['mae']\n",
    "    testing[ACCURACY] = h.history['val_mae']  # validation accuracy\n",
    "\n",
    "    epochs = range(1,epochs+1)\n",
    "    fig, axs = plt.subplots(1,2, figsize=(17,5))\n",
    "    for i, label in zip((LOSS, ACCURACY),('loss', 'mae')):   \n",
    "        axs[i].plot(epochs, training[i], 'b-', label='Training ' + label)\n",
    "        axs[i].plot(epochs, testing[i], 'y-', label='Test ' + label)\n",
    "        axs[i].set_title('Training and test ' + label)\n",
    "        axs[i].set_xlabel('Epochs')\n",
    "        axs[i].set_ylabel(label)\n",
    "        axs[i].legend()\n",
    "        axs[i].grid(True)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('1_generate_scenarios/Musical_Instruments.csv')\n",
    "\n",
    "# Map user and item IDs to integers\n",
    "user_map = {uid: idx + 1 for idx, uid in enumerate(data['user_id'].unique())}\n",
    "item_map = {iid: idx + 1 for idx, iid in enumerate(data['parent_asin'].unique())}\n",
    "\n",
    "data['user_id_num'] = data['user_id'].map(user_map)\n",
    "data['item_id'] = data['parent_asin'].map(item_map)\n",
    "\n",
    "# Count users/items\n",
    "NUM_USERS = len(user_map)\n",
    "NUM_ITEMS = len(item_map)\n",
    "\n",
    "print('Number of users:', NUM_USERS)\n",
    "print('Number of items:', NUM_ITEMS)\n",
    "\n",
    "# Prepare data for the model\n",
    "data = data[['user_id_num', 'item_id', 'rating']]\n",
    "\n",
    "# Split\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=50)\n",
    "\n",
    "# Convert to float32 arrays for Keras\n",
    "train = train.to_numpy(dtype=np.float32)\n",
    "test = test.to_numpy(dtype=np.float32)\n",
    "\n",
    "# Define index positions\n",
    "USER = 0\n",
    "ITEM = 1\n",
    "RATING = 2\n"
   ],
   "id": "6e028685eb71106",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#\n",
    "# Running DEEPMF to obtain the embedding weights (both users and items)\n",
    "#\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dropout, Dense, Concatenate, Dot\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "latent_dim = 5  # 5 neurons in the embedding can adequately code both the user and items\n",
    "\n",
    "item_input = Input(shape=[1],name='item-input')\n",
    "item_embedding = Embedding(NUM_ITEMS + 1, latent_dim, name='item-embedding')(item_input)\n",
    "item_vec = Flatten(name='item-flatten')(item_embedding)\n",
    "\n",
    "user_input = Input(shape=[1],name='user-input')\n",
    "user_embedding = Embedding(NUM_USERS + 1, latent_dim, name='user-embedding')(user_input)\n",
    "user_vec = Flatten(name='user-flatten')(user_embedding)\n",
    "\n",
    "dot = Dot(axes=1,name='item-user-concat')([item_vec, user_vec])\n",
    "\n",
    "model_deepMF = Model([user_input, item_input], dot)\n",
    "model_deepMF.compile(optimizer='adam', metrics=['mae'], loss='mean_squared_error')\n",
    "\n",
    "model_deepMF.summary()\n",
    "\n",
    "EPOCHS = 10\n",
    "history_deepMF = model_deepMF.fit([train[:,USER],train[:,ITEM]],train[:,RATING], \n",
    "                    validation_data=([test[:,USER],test[:,ITEM]], test[:,RATING]), \n",
    "                    epochs=EPOCHS, verbose=1)\n",
    "plot(history_deepMF,EPOCHS)"
   ],
   "id": "2f8b7352b3ccfcc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "model_deepMF.save('ModelDeepMFV2.keras')"
   ],
   "id": "e36ab08e16eb68a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# we create two models, from model_deepMF, to get user's and item's embeddings\n",
    "model_user_embeddings = Model(inputs=user_input, outputs=user_embedding)\n",
    "model_item_embeddings = Model(inputs=item_input, outputs=item_embedding)\n",
    "\n",
    "# obtaining all the existing users an items activation maps\n",
    "user_embeddings = model_user_embeddings.predict(np.array(range(NUM_USERS+1)))\n",
    "item_embeddings = model_item_embeddings.predict(np.array(range(NUM_ITEMS+1)))\n",
    "    "
   ],
   "id": "879a2d9cb55efe19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "#\n",
    "# creates the dataset of real samples: <user embedding, item embedding, rating>\n",
    "#\n",
    "def get_dataset(data):\n",
    "    embedding_dataset = np.zeros((len(data)+1,latent_dim * 2)) \n",
    "    ratings = np.zeros((len(data)+1))\n",
    "    for i in range(len(data)):\n",
    "        user_embedding = user_embeddings[int(data[i,USER])][0]\n",
    "        item_embedding = item_embeddings[int(data[i,ITEM])][0]\n",
    "        ratings[i] = (data[i,RATING]- 3.0) / 4.0   # normalized -0.5 to 0.5\n",
    "        embedding_dataset[i] = np.concatenate((user_embedding, item_embedding))\n",
    "    # add ratings\n",
    "    embedding_dataset = np.insert(embedding_dataset, latent_dim * 2, ratings, axis=1)\n",
    "    return embedding_dataset\n",
    "\n",
    "# train and test embeddings and ratings old_datasets (positive values)\n",
    "embedding_dataset = get_dataset(np.array(data))"
   ],
   "id": "bb8689ea4bd1d128",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " #\n",
    "# GAN to create the fake samples.\n",
    "# Both the generator and the discriminator models are really small because the source samples are not\n",
    "# large and sparse vectors; they are small and dense: 5 real number to code the user, 5 real numbers to \n",
    "# code the item and a real number to code the normalized rating.\n",
    "#\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dropout, Dense, Concatenate, Dot, LeakyReLU, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.noise_dim = 100\n",
    "\n",
    "        optimizer_d = Adam(0.0006, 0.5)\n",
    "        optimizer_g = Adam(0.00001, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer_d,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates user\n",
    "        z = Input(shape=(self.noise_dim,))\n",
    "        fake_sample = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated users as input and determines validity\n",
    "        validity = self.discriminator(fake_sample)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer_g)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_dim=self.noise_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(20, input_dim=self.noise_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(self.latent_dim*2+1, activation='linear'))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.noise_dim,))\n",
    "        fake_sample = model(noise)\n",
    "\n",
    "        return Model(noise, fake_sample)\n",
    "\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(4, input_dim=self.latent_dim*2 + 1))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        sample = Input(shape=(self.latent_dim*2 + 1,))\n",
    "        validity = model(sample)\n",
    "\n",
    "        return Model(sample, validity)\n",
    "\n",
    "    \n",
    "    def train(self, dataset, epochs, batch_size=128, sample_interval=50, num_training_samples=100000):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of real samples\n",
    "            idx = np.random.randint(0, num_training_samples, batch_size)\n",
    "            real_samples = dataset[idx]\n",
    "           \n",
    "            noise = np.random.normal(0, 1, (batch_size, self.noise_dim))\n",
    "\n",
    "            # Generate a batch of fake users\n",
    "            fake_samples = self.generator.predict(noise, verbose=0)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(real_samples, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(fake_samples, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.noise_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            if epoch % sample_interval == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                noise = np.random.normal(0, 1, (1, self.noise_dim))\n",
    "            \n",
    "            if epoch > 0 and epoch % 2000 == 0:\n",
    "                filename = f\"gan_generator_epoch{epoch}.h5\"\n",
    "                self.generator.save(filename)\n",
    "                print(f\"âœ… Generator gespeichert: {filename}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(embedding_dataset, epochs=20000, batch_size=32, sample_interval=200, \n",
    "              num_training_samples = len(embedding_dataset))\n",
    "    gan.generator.save('GANRS.h5')\n",
    "    "
   ],
   "id": "59a8523a921bea27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6f1477d8a69b1b14",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
